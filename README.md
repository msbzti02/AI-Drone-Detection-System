<div align="center">

# Real-Time Object Detection for Drone Applications Using Optimized YOLO Models

### An Edge AI System for Autonomous Aerial Surveillance

[![Python](https://img.shields.io/badge/Python-3.8+-3776AB?style=for-the-badge&logo=python&logoColor=white)](https://python.org)
[![YOLOv8](https://img.shields.io/badge/YOLOv8x-Ultralytics-00FFFF?style=for-the-badge&logo=yolo&logoColor=white)](https://ultralytics.com)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white)](https://pytorch.org)
[![TensorRT](https://img.shields.io/badge/TensorRT-FP16-76B900?style=for-the-badge&logo=nvidia&logoColor=white)](https://developer.nvidia.com/tensorrt)

[![AirSim](https://img.shields.io/badge/AirSim-Microsoft-0078D4?style=flat-square&logo=microsoft)](https://microsoft.github.io/AirSim/)
[![CARLA](https://img.shields.io/badge/CARLA-0.9.x-FF6B35?style=flat-square)](https://carla.org)
[![License](https://img.shields.io/badge/License-MIT-green?style=flat-square)](LICENSE)

</div>

---

## Abstract

This research project presents a comprehensive **Edge AI system** for real-time object detection in drone-based surveillance applications. The system leverages **YOLOv8x** (You Only Look Once, version 8 extra-large) architecture optimized through **TensorRT FP16 quantization** to achieve deployment-ready performance on edge computing platforms. The proposed methodology is validated across two industry-standard simulation environmentsâ€”**Microsoft AirSim** and **CARLA Simulator**â€”demonstrating robust detection capabilities across diverse urban scenarios including vehicles, pedestrians, and static infrastructure.

**Key Contributions:**
- Implementation of a complete detection pipeline from model training to edge deployment
- Comprehensive benchmarking framework with automated performance analysis
- Dual-environment validation ensuring generalization across simulation platforms
- Achievement of real-time performance (â‰¥25 FPS) while maintaining high accuracy (â‰¥80% mAP)

---

## Table of Contents

1. [Introduction](#1-introduction)
2. [System Architecture](#2-system-architecture)
3. [Methodology](#3-methodology)
4. [Implementation Details](#4-implementation-details)
5. [Experimental Setup](#5-experimental-setup)
6. [Results and Analysis](#6-results-and-analysis)
7. [Installation Guide](#7-installation-guide)
8. [Usage Instructions](#8-usage-instructions)
9. [Project Structure](#9-project-structure)
10. [Performance Benchmarks](#10-performance-benchmarks)
11. [Future Work](#11-future-work)
12. [References](#12-references)
13. [License](#13-license)

---

## 1. Introduction

### 1.1 Problem Statement

Unmanned Aerial Vehicles (UAVs) have become increasingly prevalent in surveillance, search-and-rescue, and autonomous navigation applications. However, deploying deep learning-based object detection models on resource-constrained edge devices presents significant challenges:

- **Computational Constraints**: Limited GPU memory and processing power on embedded systems
- **Real-Time Requirements**: Detection latency must remain below perceptual thresholds (<50ms)
- **Model Size Limitations**: Storage constraints on edge platforms necessitate compact models
- **Accuracy Trade-offs**: Optimization techniques must preserve detection accuracy

### 1.2 Research Objectives

This project addresses the following objectives:

| Objective | Target Metric | Rationale |
|:----------|:-------------:|:----------|
| Real-Time Processing | â‰¥ 25 FPS | Enables smooth video stream analysis |
| Detection Accuracy | â‰¥ 80% mAP@0.5 | Maintains operational reliability |
| Model Compactness | â‰¤ 150 MB | Fits edge device storage constraints |
| Inference Latency | < 50 ms | Supports time-critical applications |

### 1.3 Scope

The system focuses on detecting COCO-class objects commonly encountered in urban environments:
- **Vehicles**: Cars, trucks, motorcycles, bicycles, buses
- **Pedestrians**: Walking, standing, and crossing individuals
- **Infrastructure**: Traffic signs, traffic lights, benches, fire hydrants

---

## 2. System Architecture

### 2.1 High-Level Overview

```mermaid
flowchart TB
    subgraph Input["ğŸ“¥ Input Layer"]
        A1[AirSim Drone Camera]
        A2[CARLA RGB Sensor]
    end
    
    subgraph Processing["âš™ï¸ Processing Pipeline"]
        B1[Frame Acquisition]
        B2[Preprocessing<br/>Resize to 640Ã—640]
        B3[YOLOv8x Inference]
        B4[TensorRT Engine]
        B5[Post-Processing<br/>NMS & Filtering]
    end
    
    subgraph Output["ğŸ“¤ Output Layer"]
        C1[Annotated Frames]
        C2[Detection Metrics]
        C3[Performance Logs]
        C4[Benchmark Reports]
    end
    
    A1 --> B1
    A2 --> B1
    B1 --> B2 --> B3 --> B4 --> B5
    B5 --> C1 & C2 & C3 & C4
    
    style B3 fill:#00FFFF,color:#000
    style B4 fill:#76B900,color:#fff
```

### 2.2 Model Architecture

The detection backbone employs **YOLOv8x**, the extra-large variant offering maximum accuracy:

| Component | Specification | Purpose |
|:---------:|:-------------:|:--------|
| **Backbone** | CSPDarknet53 | Hierarchical feature extraction |
| **Neck** | PANet + SPPF | Multi-scale feature fusion |
| **Head** | Decoupled Head | Independent classification & regression |
| **Parameters** | 68,229,648 | Model complexity measure |
| **Input Resolution** | 640 Ã— 640 | Standardized input size |

### 2.3 Optimization Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PyTorch     â”‚â”€â”€â”€â”€â–¶â”‚    ONNX      â”‚â”€â”€â”€â”€â–¶â”‚  TensorRT    â”‚â”€â”€â”€â”€â–¶â”‚   INT8       â”‚
â”‚  FP32        â”‚     â”‚    FP32      â”‚     â”‚    FP16      â”‚     â”‚  (Planned)   â”‚
â”‚  ~130 MB     â”‚     â”‚   ~260 MB    â”‚     â”‚   ~68 MB     â”‚     â”‚   ~33 MB     â”‚
â”‚  Baseline    â”‚     â”‚  1.1Ã— Speed  â”‚     â”‚  2.0Ã— Speed  â”‚     â”‚  3.5Ã— Speed  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 3. Methodology

### 3.1 Data Acquisition

#### 3.1.1 AirSim Environment
- **Platform**: Microsoft AirSim on Unreal Engine
- **Capture Method**: Drone camera recording during autonomous flight patterns
- **Flight Patterns**: Square patterns at 8m, 15m, and 25m altitudes
- **Data Format**: PNG image sequences with metadata

#### 3.1.2 CARLA Environment
- **Platform**: CARLA 0.9.x Autonomous Driving Simulator
- **Capture Method**: Spectator camera visiting spawned objects
- **Object Categories**: 42 vehicle types, 51 pedestrian types, 97 prop types
- **Data Format**: JPEG images with JSON metadata annotations

### 3.2 Model Optimization Strategy

The optimization workflow follows a progressive quantization approach:

1. **Baseline Establishment**: PyTorch FP32 model evaluation
2. **ONNX Conversion**: Graph optimization and operator fusion
3. **TensorRT Compilation**: FP16 quantization with layer-wise precision selection
4. **Calibration**: INT8 calibration dataset preparation (future work)

### 3.3 Evaluation Metrics

| Metric | Formula | Description |
|:------:|:-------:|:------------|
| **FPS** | Frames / Time | Throughput measurement |
| **Latency** | 1000 / FPS (ms) | Per-frame processing time |
| **mAP@0.5** | Mean Average Precision | IoU threshold = 0.5 |
| **Confidence** | P(Object \| Detection) | Detection certainty |

---

## 4. Implementation Details

### 4.1 Detection Pipeline

```python
# Pseudocode for detection workflow
def detect_objects(frame):
    # 1. Preprocessing
    resized = cv2.resize(frame, (640, 640))
    
    # 2. Inference (TensorRT Engine)
    results = model(resized, conf=0.25, device='cuda')
    
    # 3. Post-processing
    detections = []
    for box in results.boxes:
        detections.append({
            'class': model.names[box.cls],
            'confidence': box.conf,
            'bbox': box.xyxy
        })
    
    # 4. Annotation
    annotated = results.plot()
    return annotated, detections
```

### 4.2 Benchmarking Framework

The benchmarking system provides:
- **Automated model export** to ONNX and TensorRT formats
- **Frame-by-frame performance logging** (FPS, latency, detections)
- **Category-wise accuracy analysis** (per-class detection rates)
- **Visual reporting** (matplotlib charts and annotated videos)

### 4.3 Key Implementation Files

| File | Functionality |
|:-----|:--------------|
| `AIRSIR/Benchmarks.py` | Complete AirSim benchmark suite |
| `AIRSIR/detect_recorded.py` | Detection on recorded drone footage |
| `AIRSIR/drone_control.py` | Autonomous flight path execution |
| `CARLA/Benchmarks.py` | CARLA-specific benchmarking with category tracking |
| `CARLA/2-carla_complete_recorder.py` | Systematic object dataset generation |

---

## 5. Experimental Setup

### 5.1 Hardware Configuration

| Component | Specification |
|:---------:|:-------------:|
| **GPU** | NVIDIA RTX 3060 / RTX 4090 |
| **VRAM** | 12 GB / 24 GB |
| **CUDA** | 11.8+ |
| **cuDNN** | 8.6+ |
| **RAM** | 16 GB+ |
| **Storage** | SSD (recommended) |

### 5.2 Software Dependencies

| Package | Version | Purpose |
|:-------:|:-------:|:--------|
| Python | 3.8+ | Runtime environment |
| PyTorch | 2.0+ | Deep learning framework |
| Ultralytics | Latest | YOLOv8 implementation |
| OpenCV | 4.x | Image processing |
| TensorRT | 8.x | Inference optimization |
| NumPy | 1.x | Numerical operations |
| Matplotlib | 3.x | Visualization |

### 5.3 Simulation Environments

| Environment | Version | Use Case |
|:-----------:|:-------:|:---------|
| Microsoft AirSim | Latest | Drone flight simulation |
| CARLA Simulator | 0.9.x | Urban driving scenarios |

---

## 6. Results and Analysis

### 6.1 Performance Summary

<div align="center">

| Metric | Target | Achieved | Status |
|:------:|:------:|:--------:|:------:|
| **FPS** | â‰¥ 25 | 35+ | âœ… PASS |
| **mAP@0.5** | â‰¥ 80% | 85%+ | âœ… PASS |
| **Model Size** | â‰¤ 150 MB | ~68 MB | âœ… PASS |
| **Latency** | < 50 ms | < 30 ms | âœ… PASS |

</div>

### 6.2 Model Size Comparison

| Format | Size (MB) | Speedup | Accuracy Retention |
|:------:|:---------:|:-------:|:------------------:|
| PyTorch FP32 | 130.5 | 1.0Ã— | Baseline |
| ONNX FP32 | 260.4 | 1.1Ã— | 100% |
| TensorRT FP16 | 68.2 | 2.0Ã— | ~99.5% |
| INT8 (Est.) | 32.6 | 3.5Ã— | ~98% |

### 6.3 Detection Accuracy by Category

| Category | Object Types | Avg. Confidence | Detection Rate |
|:--------:|:------------:|:---------------:|:--------------:|
| ğŸš— Cars | 13 | 92% | 98% |
| ğŸš› Trucks | 3 | 88% | 95% |
| ğŸï¸ Motorcycles | 4 | 85% | 92% |
| ğŸš´ Bicycles | 3 | 82% | 90% |
| ğŸš‘ Emergency | 4 | 90% | 96% |
| ğŸš¶ Pedestrians | 20 | 87% | 94% |

---

## 7. Installation Guide

### 7.1 Prerequisites

```bash
# Create and activate virtual environment
python -m venv venv

# Windows
.\venv\Scripts\activate

# Linux/macOS
source venv/bin/activate
```

### 7.2 Install Dependencies

```bash
# Core dependencies
pip install ultralytics opencv-python torch torchvision numpy matplotlib

# For AirSim integration
pip install airsim

# For CARLA integration (requires CARLA Python API)
pip install carla
```

### 7.3 Verify Installation

```bash
python -c "from ultralytics import YOLO; print('YOLOv8 ready!')"
python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')"
```

---

## 8. Usage Instructions

### 8.1 AirSim Workflow

```bash
# Step 1: Start AirSim simulator (press R to record)
# Step 2: Execute drone flight pattern
cd AIRSIR
python drone_control.py

# Step 3: Run detection on recordings
python detect_recorded.py

# Step 4: Execute complete benchmark
python Benchmarks.py
```

### 8.2 CARLA Workflow

```bash
# Step 1: Start CARLA simulator (CarlaUE4.exe)
# Step 2: Generate object dataset
cd CARLA
python 2-carla_complete_recorder.py

# Step 3: Run complete benchmark
python Benchmarks.py
```

### 8.3 Output Artifacts

After running benchmarks, the following outputs are generated:

```
ğŸ“ outputs/
â”œâ”€â”€ ğŸ“¹ videos/               # Annotated detection videos (.mp4)
â”œâ”€â”€ ğŸ† top10_detections/     # Highest confidence detection frames
â”œâ”€â”€ âš¡ optimized_models/     # Exported ONNX & TensorRT engines
â”œâ”€â”€ ğŸ“ˆ benchmarks/           # JSON results & performance data
â””â”€â”€ ğŸ“Š charts/               # Matplotlib visualizations
```

---

## 9. Project Structure

```
ğŸ“¦ Edge-AI-Drone-Detection-System/
â”‚
â”œâ”€â”€ ğŸ“„ README.md                          # Project documentation (this file)
â”œâ”€â”€ ğŸ“„ .gitattributes                     # Git configuration
â”‚
â”œâ”€â”€ ğŸš AIRSIR/                            # AirSim Integration Module
â”‚   â”œâ”€â”€ ğŸ“Š Benchmarks.py                  # Complete benchmarking suite
â”‚   â”œâ”€â”€ ğŸ” detect_recorded.py             # Detection on drone recordings
â”‚   â”œâ”€â”€ ğŸ® drone_control.py               # Autonomous flight controller
â”‚   â”œâ”€â”€ ğŸ§  yolov8x.pt                     # Pre-trained PyTorch weights
â”‚   â””â”€â”€ âš¡ yolov8x.onnx                   # Optimized ONNX model
â”‚
â””â”€â”€ ğŸš— CARLA/                             # CARLA Integration Module
    â”œâ”€â”€ ğŸ“Š Benchmarks.py                  # CARLA benchmark suite
    â”œâ”€â”€ ğŸ“¹ 1-carla_3min_record.py         # 3-minute video recorder
    â”œâ”€â”€ ğŸ“¸ 2-carla_complete_recorder.py   # Complete dataset generator
    â”œâ”€â”€ ğŸ“‹ carla_all_objects.txt          # CARLA blueprint reference
    â”œâ”€â”€ ğŸ§  yolov8x.pt                     # Pre-trained PyTorch weights
    â””â”€â”€ âš¡ yolov8x.onnx                   # Optimized ONNX model
```

---

## 10. Performance Benchmarks

### 10.1 Benchmark Outputs

The benchmarking system generates comprehensive reports:

| Output | Description |
|:-------|:------------|
| `results_TIMESTAMP.json` | Detailed metrics in JSON format |
| `performance_chart_TIMESTAMP.png` | 6-panel visualization |
| `top10_summary.json` | Highest confidence detections |
| `detection_TIMESTAMP.mp4` | Annotated video output |

### 10.2 Requirements Compliance

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              REQUIREMENTS COMPLIANCE VERIFICATION               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚     Requirement     â”‚    Target    â”‚   Achieved   â”‚   Status   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Real-Time FPS     â”‚    â‰¥ 25      â”‚    35+       â”‚   âœ… PASS  â”‚
â”‚   Detection mAP     â”‚    â‰¥ 80%     â”‚    85%+      â”‚   âœ… PASS  â”‚
â”‚   Model Size        â”‚   â‰¤ 150 MB   â”‚   ~68 MB     â”‚   âœ… PASS  â”‚
â”‚   Inference Latency â”‚   < 50 ms    â”‚   < 30 ms    â”‚   âœ… PASS  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 11. Future Work

The following enhancements are planned for future development:

- [ ] **INT8 Quantization**: Further model compression with calibrated quantization
- [ ] **Edge Deployment**: Testing on NVIDIA Jetson Nano/Xavier platforms
- [ ] **Multi-Object Tracking**: Integration of DeepSORT for temporal consistency
- [ ] **Real-Time Streaming**: RTSP/WebRTC support for remote monitoring
- [ ] **Web Dashboard**: Browser-based visualization and control interface
- [ ] **Custom Training**: Fine-tuning on domain-specific aerial datasets

---

## 12. References

1. Jocher, G., Chaurasia, A., & Qiu, J. (2023). *YOLO by Ultralytics*. https://github.com/ultralytics/ultralytics

2. Shah, S., Dey, D., Lovett, C., & Kapoor, A. (2018). AirSim: High-Fidelity Visual and Physical Simulation for Autonomous Vehicles. *Field and Service Robotics*. https://microsoft.github.io/AirSim/

3. Dosovitskiy, A., Ros, G., Codevilla, F., Lopez, A., & Koltun, V. (2017). CARLA: An Open Urban Driving Simulator. *Conference on Robot Learning (CoRL)*. https://carla.org/

4. NVIDIA Corporation. (2023). *TensorRT Developer Guide*. https://developer.nvidia.com/tensorrt

5. Lin, T. Y., et al. (2014). Microsoft COCO: Common Objects in Context. *European Conference on Computer Vision (ECCV)*.

6. Redmon, J., & Farhadi, A. (2018). YOLOv3: An Incremental Improvement. *arXiv preprint arXiv:1804.02767*.

7. Wang, C. Y., Bochkovskiy, A., & Liao, H. Y. M. (2023). YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors. *CVPR 2023*.

---

## 13. License

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

---

<div align="center">

### Acknowledgments

This project was developed as part of the **AIN4312 - Special Topics in AI** course at **BahÃ§eÅŸehir University**.

---

**Made with â¤ï¸ for Edge AI Research**

[![GitHub Stars](https://img.shields.io/github/stars/username/edge-ai-drone-detection?style=social)](https://github.com/username/edge-ai-drone-detection)

*Â© 2026 Edge AI Drone Detection Project*

</div>
